import os
import numpy as np
import pandas as pd
import torch
import matplotlib.pyplot as plt
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from wordcloud import WordCloud
from tqdm import tqdm

# ======================================================
# 0ï¸âƒ£ ë””ë ‰í† ë¦¬ ì„¤ì •
# ======================================================
OUTPUT_DIR = "outputs/step3/core_keyword_analysis_startup_llm"
WORDCLOUD_DIR = os.path.join(OUTPUT_DIR, "wordclouds")
KEYWORD_DIR = os.path.join(OUTPUT_DIR, "keywords")

os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(WORDCLOUD_DIR, exist_ok=True)
os.makedirs(KEYWORD_DIR, exist_ok=True)

print(f"âœ… Output directories created:")
print(f"   - Main: {OUTPUT_DIR}")
print(f"   - Wordclouds: {WORDCLOUD_DIR}")
print(f"   - Keywords: {KEYWORD_DIR}\n")

# ğŸ”§ ì¶”ê°€: êµ¬ì–´ì²´ ë° ë¶ˆí•„ìš” ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸
CUSTOM_STOPWORDS = {
    # ê¸°ë³¸ ë¶ˆìš©ì–´
    'like', 'know', 'just', 'think', 'really', 'going', 'yeah', 'want', 'got', 'getting',
    # êµ¬ì–´ì²´ ì‚½ì…ì–´
    'uh', 'um', 'ah', 'er', 'hmm', 'mm', 'oh',
    # ì¼ë°˜ì ì¸ ë‹¨ì–´
    'thing', 'things', 'people', 'time', 'way', 'right', 'kind', 've', 'll', 'don',
    # ëŒ€í™” ê´€ë ¨
    'say', 'said', 'tell', 'told', 'talk', 'talking', 'mean', 'feel', 'feeling',
    # ê¸°íƒ€ ì¼ë°˜ì–´
    'good', 'bad', 'better', 'best', 'lot', 'little', 'big', 'small',
    'day', 'year', 'years', 'today', 'didn', 'doesn', 'wasn', 'weren',
    # ë¶ˆì™„ì „ ë‹¨ì–´
    'gt', 'gt gt', 'thank', 'thanks'
}

# ======================================================
# 1ï¸âƒ£ ëª¨ë¸ ë¡œë“œ
# ======================================================
def load_model():
    MODEL_ID = "google/gemma-2-9b-it"
    print(f"ğŸš€ Loading model: {MODEL_ID}")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16,
        low_cpu_mem_usage=True
    ).to("cuda:0")
    
    generator = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        device=torch.device("cuda:0"),
        max_new_tokens=300,
        do_sample=False
    )
    return generator

# ======================================================
# 2ï¸âƒ£ ê°œì„ ëœ TF-IDF í‚¤ì›Œë“œ ì¶”ì¶œ
# ======================================================
def extract_keywords_tfidf(texts, top_n=30):
    """TF-IDFë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        # ê¸°ë³¸ ì˜ì–´ ë¶ˆìš©ì–´ + ì»¤ìŠ¤í…€ ë¶ˆìš©ì–´ ê²°í•©
        from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS
        all_stopwords = list(ENGLISH_STOP_WORDS.union(CUSTOM_STOPWORDS))
        
        tfidf = TfidfVectorizer(
            stop_words=all_stopwords,
            max_features=3000,
            min_df=3,  # ìµœì†Œ 3ê°œ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ì•¼ í•¨
            max_df=0.7,  # 70% ì´ìƒ ë¬¸ì„œì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¨ì–´ ì œì™¸
            ngram_range=(1, 3),  # 1-gramë¶€í„° 3-gramê¹Œì§€
            token_pattern=r'\b[a-zA-Z]{3,}\b'  # ìµœì†Œ 3ê¸€ì ì´ìƒ ë‹¨ì–´ë§Œ
        )
        
        tfidf_matrix = tfidf.fit_transform(texts)
        feature_names = np.array(tfidf.get_feature_names_out())
        scores = tfidf_matrix.mean(axis=0).A1
        
        # ğŸ”§ ì¶”ê°€ í•„í„°ë§: ìˆ«ìë§Œ ìˆëŠ” ë‹¨ì–´ ì œê±°
        valid_indices = []
        for idx, word in enumerate(feature_names):
            # ìˆ«ìë§Œ ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ì€ ë‹¨ì–´ ì œì™¸
            if not word.replace(' ', '').isdigit() and len(word.strip()) >= 3:
                valid_indices.append(idx)
        
        if len(valid_indices) == 0:
            return []
        
        feature_names = feature_names[valid_indices]
        scores = scores[valid_indices]
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
        top_indices = scores.argsort()[-top_n:][::-1]
        top_keywords = feature_names[top_indices]
        top_scores = scores[top_indices]
        
        return list(zip(top_keywords, top_scores))
    except Exception as e:
        print(f"   âš ï¸ TF-IDF failed: {e}")
        return []

# ======================================================
# 3ï¸âƒ£ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
# ======================================================
def create_wordcloud(keywords_with_scores, cluster_id, cluster_size):
    """í‚¤ì›Œë“œë¡œ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±"""
    if len(keywords_with_scores) == 0:
        return None
    
    word_freq = dict(keywords_with_scores)
    
    wc = WordCloud(
        width=1200,
        height=600,
        background_color='white',
        colormap='viridis',
        relative_scaling=0.5,
        min_font_size=10
    ).generate_from_frequencies(word_freq)
    
    plt.figure(figsize=(15, 7))
    plt.imshow(wc, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Cluster {cluster_id} Key Topics ({cluster_size} sentences)', 
              fontsize=16, fontweight='bold', pad=20)
    plt.tight_layout()
    
    output_path = os.path.join(WORDCLOUD_DIR, f'cluster_{cluster_id}_wordcloud.png')
    plt.savefig(output_path, dpi=200, bbox_inches='tight')
    plt.close()
    
    return output_path

# ======================================================
# 4ï¸âƒ£ í‚¤ì›Œë“œ ë°” ì°¨íŠ¸ ìƒì„±
# ======================================================
def create_keyword_chart(keywords_with_scores, cluster_id, cluster_size):
    """ìƒìœ„ í‚¤ì›Œë“œë¥¼ ë°” ì°¨íŠ¸ë¡œ ì‹œê°í™”"""
    if len(keywords_with_scores) == 0:
        return None
    
    # ìƒìœ„ 15ê°œë§Œ ì„ íƒ
    top_15 = keywords_with_scores[:15]
    keywords = [kw for kw, _ in top_15]
    scores = [score for _, score in top_15]
    
    # ë°” ì°¨íŠ¸ ìƒì„±
    plt.figure(figsize=(12, 8))
    bars = plt.barh(range(len(keywords)), scores, color='steelblue', alpha=0.8)
    plt.yticks(range(len(keywords)), keywords, fontsize=11)
    plt.xlabel('TF-IDF Score', fontsize=12)
    plt.title(f'Cluster {cluster_id} Top Topics ({cluster_size} sentences)', 
              fontsize=14, fontweight='bold', pad=15)
    plt.gca().invert_yaxis()
    
    # ê°’ í‘œì‹œ
    for i, (bar, score) in enumerate(zip(bars, scores)):
        plt.text(score, i, f' {score:.3f}', va='center', fontsize=9)
    
    plt.tight_layout()
    
    output_path = os.path.join(KEYWORD_DIR, f'cluster_{cluster_id}_keywords.png')
    plt.savefig(output_path, dpi=200, bbox_inches='tight')
    plt.close()
    
    return output_path

# ======================================================
# 5ï¸âƒ£ í´ëŸ¬ìŠ¤í„° ì£¼ì œ ë° ê°€ì¹˜ê´€ ë¶„ì„
# ======================================================
def analyze_cluster_theme(generator, texts, cluster_id, max_samples=20):
    """LLMì„ ì‚¬ìš©í•œ ì£¼ì œ ë° ê°€ì¹˜ê´€ ë¶„ì„"""
    if len(texts) > max_samples:
        sample_texts = np.random.choice(texts, max_samples, replace=False)
    else:
        sample_texts = texts
    
    sentences_list = "\n".join([f"{i+1}. {t[:200]}" for i, t in enumerate(sample_texts)])
    
    prompt = f"""You are an expert in thematic analysis and value extraction.

Analyze the following sentences from Cluster {cluster_id} and provide:

1. **Main Theme**: What is the central topic or subject matter? (1-2 sentences)
2. **Core Values**: What values, beliefs, or perspectives are expressed? (1-2 sentences)
3. **Summary**: Summarize the key message in simple terms (2-3 sentences)

Sentences:
{sentences_list}

Analysis:
1. Main Theme:"""
    
    output = generator(prompt, max_new_tokens=300)[0]["generated_text"]
    
    if "Analysis:" in output:
        analysis = output.split("Analysis:")[-1].strip()
    else:
        analysis = output.split(prompt)[-1].strip() if prompt in output else output
    
    return analysis

# ======================================================
# 6ï¸âƒ£ ë©”ì¸ ì½”ë“œ
# ======================================================
print("ğŸ”¹ Loading data...")
df = pd.read_csv("outputs/step3/all_sentences_with_umap_startup_llm.csv")
df = df.dropna(subset=["text"])

n_clusters = len(df["cluster"].unique()) - (1 if -1 in df["cluster"].unique() else 0)
print(f"ğŸ“Š Found {n_clusters} clusters in the data\n")

# ëª¨ë¸ ë¡œë“œ
generator = load_model()

# ê²°ê³¼ ì €ì¥ìš©
cluster_analyses = []

print("\nğŸ”¹ Analyzing clusters...\n")
for cid in tqdm(sorted(df["cluster"].unique()), desc="Processing clusters"):
    if cid == -1: 
        continue
    
    texts = df[df["cluster"] == cid]["text"].tolist()
    
    if len(texts) < 5:
        print(f"\nâš ï¸ Cluster {cid}: Only {len(texts)} sentences, skipping...")
        continue
    
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Cluster {cid} ({len(texts)} sentences)")
    print(f"{'='*60}")
    
    # ëŒ€í‘œ ë¬¸ì¥ ì¶œë ¥
    print("\nğŸ“ Sample sentences:")
    for i, sent in enumerate(texts[:3], 1):
        print(f"   {i}. {sent[:150]}...")
    
    # í‚¤ì›Œë“œ ì¶”ì¶œ
    print("\nğŸ”¹ Extracting meaningful keywords...")
    keywords_with_scores = extract_keywords_tfidf(texts, top_n=30)
    
    if len(keywords_with_scores) == 0:
        print("   âš ï¸ No meaningful keywords found")
        top_10_keywords = []
    else:
        top_10_keywords = [kw for kw, _ in keywords_with_scores[:10]]
        print(f"   Top 10 topics: {', '.join(top_10_keywords)}")
    
    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    wordcloud_path = create_wordcloud(keywords_with_scores, cid, len(texts))
    if wordcloud_path:
        print(f"   âœ… Wordcloud saved: {os.path.basename(wordcloud_path)}")
    
    # í‚¤ì›Œë“œ ì°¨íŠ¸ ìƒì„±
    keyword_chart_path = create_keyword_chart(keywords_with_scores, cid, len(texts))
    if keyword_chart_path:
        print(f"   âœ… Keyword chart saved: {os.path.basename(keyword_chart_path)}")
    
    # LLM ë¶„ì„
    print("\nğŸ”¹ Analyzing theme and values...")
    analysis = analyze_cluster_theme(generator, texts, cid, max_samples=20)
    
    print(f"\nğŸ” Analysis:\n{analysis}\n")
    
    # ê²°ê³¼ ì €ì¥
    cluster_analyses.append({
        "cluster_id": cid,
        "size": len(texts),
        "top_keywords": ", ".join(top_10_keywords) if top_10_keywords else "N/A",
        "sample_sentences": " | ".join(texts[:3]),
        "analysis": analysis,
        "wordcloud_path": wordcloud_path or "",
        "keyword_chart_path": keyword_chart_path or ""
    })
    
    # ê°œë³„ í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥
    with open(os.path.join(OUTPUT_DIR, f"cluster_{cid}_analysis.txt"), "w", encoding="utf-8") as f:
        f.write(f"Cluster {cid} Analysis\n")
        f.write(f"{'='*60}\n\n")
        f.write(f"Size: {len(texts)} sentences\n\n")
        
        if top_10_keywords:
            f.write(f"Top 10 Topics:\n")
            f.write(f"{', '.join(top_10_keywords)}\n\n")
        
        f.write(f"Sample Sentences:\n")
        for i, sent in enumerate(texts[:3], 1):
            f.write(f"{i}. {sent}\n\n")
        
        f.write(f"\nTheme & Values Analysis:\n{analysis}\n")

# ======================================================
# 7ï¸âƒ£ ê²°ê³¼ ì €ì¥
# ======================================================
result_df = pd.DataFrame(cluster_analyses)
csv_path = os.path.join(OUTPUT_DIR, "cluster_themes_and_values.csv")
result_df.to_csv(csv_path, index=False, encoding="utf-8-sig")

print(f"\n{'='*60}")
print(f"âœ… Analysis complete!")
print(f"âœ… Results saved to: {csv_path}")
print(f"âœ… Individual analyses: {OUTPUT_DIR}/cluster_*_analysis.txt")
print(f"âœ… Wordclouds: {WORDCLOUD_DIR}/")
print(f"âœ… Keyword charts: {KEYWORD_DIR}/")
print(f"{'='*60}\n")

# ìš”ì•½ í…Œì´ë¸” ì¶œë ¥
print(f"ğŸ“‹ Summary Table ({len(result_df)} clusters):")
for _, row in result_df.iterrows():
    print(f"\n   Cluster {row['cluster_id']:2d} ({row['size']:3d} sentences):")
    print(f"   Topics: {row['top_keywords'][:80]}{'...' if len(row['top_keywords']) > 80 else ''}")

# ì „ì²´ ìš”ì•½ íŒŒì¼ ìƒì„±
summary_path = os.path.join(OUTPUT_DIR, "all_clusters_summary.txt")
with open(summary_path, "w", encoding="utf-8") as f:
    f.write("=" * 80 + "\n")
    f.write(f"ALL CLUSTERS ANALYSIS SUMMARY ({len(result_df)} clusters)\n")
    f.write("=" * 80 + "\n\n")
    
    for _, row in result_df.iterrows():
        f.write(f"\n{'='*60}\n")
        f.write(f"Cluster {row['cluster_id']} ({row['size']} sentences)\n")
        f.write(f"{'='*60}\n\n")
        f.write(f"Top Topics: {row['top_keywords']}\n\n")
        f.write(f"Analysis:\n{row['analysis']}\n\n")

print(f"\nâœ… Summary file saved to: {summary_path}")

# í‚¤ì›Œë“œ í†µê³„
print(f"\nğŸ’¡ Analysis Statistics:")
print(f"   - Total clusters analyzed: {len(result_df)}")
print(f"   - Wordclouds generated: {len([p for p in result_df['wordcloud_path'] if p])}")
print(f"   - Keyword charts generated: {len([p for p in result_df['keyword_chart_path'] if p])}")
